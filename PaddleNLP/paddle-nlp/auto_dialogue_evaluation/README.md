# Auto Dialogue Evaluation

## 简介

### 任务说明
对话自动评估（Auto Dialogue Evaluation）评估开放领域对话系统的回复质量，能够帮助企业或个人快速评估对话系统的回复质量，减少人工评估成本。
1. 在无标注数据的情况下，利用负采样训练匹配模型作为评估工具，实现对多个对话系统回复质量排序；
2. 利用少量标注数据（特定对话系统或场景的人工打分），在匹配模型基础上进行微调，可以显著该对话系统或场景的评估效果。

### 效果说明
我们以四个不同的对话系统（seq2seq\_naive／seq2seq\_att／keywords／human）为例，使用对话自动评估工具进行自动评估。
1. 无标注数据情况下，直接使用预训练好的评估工具进行评估；
	在四个对话系统上，自动评估打分和人工评估打分spearman相关系数，如下：
	/|seq2seq\_naive|seq2seq\_att|keywords|human
	--|:--:|--:|:--:|--:
	cor|0.361|0.343|0.324|0.288
	对四个系统平均得分排序：
2. 利用少量标注数据微调后，自动评估打分和人工打分spearman相关系数，如下：
	/|seq2seq\_naive|seq2seq\_att|keywords|human
	--|:--:|--:|:--:|--:
	cor|0.474|0.477|0.443|0.378
